{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6335937c",
   "metadata": {},
   "source": [
    "# ---------------- Neural Networks & Gradient Descent Algorithm Tutorial ----------------\n",
    "\n",
    "**Author:** ClaemWu  \n",
    "**Affiliation:** HKBU AEF MscDABE  \n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial provides detailed examples explaining the concepts and working principles of neural networks, as well as the derivation of gradient descent.\n",
    "\n",
    "**Note:** This tutorial is intended for learning and reference purposes only and does not carry any academic credit or certification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc1285",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron (MLP): Feedforward Artificial Neural Network (NN)\n",
    "\n",
    "The **Multilayer Perceptron (MLP)** represents one of the simplest and most fundamental architectures in neural network design. Often considered the essential **building block** or **computational cornerstone** of modern deep learning, its elegant structure forms the basis upon which far more complex models are constructed.\n",
    "\n",
    "It comprises of:\n",
    "- An **input layer** that receives the initial data.\n",
    "- One or more **hidden layers** that perform the core computation.\n",
    "- An **output layer** that produces the final prediction or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4e081",
   "metadata": {},
   "source": [
    "# Neural Network Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ad636",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "![Network Architecture](3_layerNN.png)\n",
    "*3-Layer Neural Network*\n",
    "\n",
    "</td>\n",
    "<td width=\"50%\" valign=\"top\">\n",
    "\n",
    "### Matrix Dimension Flow\n",
    "The dimensions ensure compatible operations in the forward pass:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "\\text{Input:} & \\quad \\mathbf{X} && (m, 2) \\\\\n",
    "\\text{Layer 1:} & \\quad \\mathbf{Z}^{[1]} = \\mathbf{X} (\\mathbf{W}^{(1)})^\\top && \\rightarrow (m, 3) \\\\\n",
    "& \\quad \\mathbf{A}^{[1]} = \\sigma(\\mathbf{Z}^{[1]}) && \\rightarrow (m, 3) \\\\\n",
    "\\text{Layer 2:} & \\quad \\mathbf{Z}^{[2]} = \\mathbf{A}^{[1]} (\\mathbf{W}^{(2)})^\\top && \\rightarrow (m, 3) \\\\\n",
    "& \\quad \\mathbf{A}^{[2]} = \\sigma(\\mathbf{Z}^{[2]}) && \\rightarrow (m, 3) \\\\\n",
    "\\text{Output:} & \\quad \\mathbf{Z}^{[3]} = \\mathbf{A}^{[2]} (\\mathbf{W}^{(3)})^\\top && \\rightarrow (m, 1) \\\\\n",
    "& \\quad \\mathbf{A}^{[last]} = \\mathbf{\\hat{Y}} = \\sigma(\\mathbf{Z}^{[3]}) && \\rightarrow (m, 1)\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b07518",
   "metadata": {},
   "source": [
    "## Model Parameter Specification\n",
    "\n",
    "This section defines the key weight matrices for the 3-layer neural network.\n",
    "\n",
    "**Layer 1 (Input to Hidden Layer):**\n",
    "*   **Weight Matrix:** $\\mathbf{W}^{(1)}$\n",
    "*   **Dimensions:** $3 \\times 2$\n",
    "*   **Description:** This matrix connects the $2$ input neurons to the $3$ neurons in the first hidden layer. Each element $w^{(1)}_{ij}$ represents the weight from input neuron $j$ 1~2 to hidden neuron $i$ 1~3.\n",
    "\n",
    "*   **Activation Function (Sigmoid):**\n",
    "    \\[\n",
    "    $\\mathbf{A}^{[1]} = \\sigma(\\mathbf{Z}^{[1]}) = \\frac{1}{1 + e^{-\\mathbf{Z}^{[1]}}}$\n",
    "    \\]\n",
    "    The sigmoid function transforms each element $z^{[1]}_{ij}$ of the linear output $\\mathbf{Z}^{[1]}$ into an activation value $a^{[1]}_{ij}$ between 0 and 1. This introduces non-linearity, allowing the network to learn complex patterns. $\\mathbf{A}^{[1]}$ has shape $(m, 3)$.\n",
    "\n",
    "**Layer 2 (Hidden to Hidden):**\n",
    "*   **Weight Matrix:** $\\mathbf{W}^{(2)}$\n",
    "*   **Dimensions:** $3 \\times 3$\n",
    "*   **Description:** This matrix connects the $3$ hidden layer1 neurons to the $3$ hidden layer2 neurons. Each element $w^{(2)}_{ij}$ represents the weight from layer1 neuron $j$ 1~3 to layer2 neuron $i$ 1~3.\n",
    "*   **Activation Function (Sigmoid):**\n",
    "    \\[\n",
    "    $\\mathbf{A}^{[2]} = \\sigma(\\mathbf{Z}^{[2]}) = \\frac{1}{1 + e^{-\\mathbf{Z}^{[2]}}}$\n",
    "    \\]\n",
    "    Applies the same sigmoid activation to $\\mathbf{Z}^{[2]}$, producing the second hidden layer's activated output $\\mathbf{A}^{[2]}$ with shape $(m, 3)$.\n",
    "\n",
    "**Layer 3 (Hidden to Output):**\n",
    "*   **Weight Matrix:** $\\mathbf{W}^{(3)}$\n",
    "*   **Dimensions:** $1 \\times 3$\n",
    "*   **Description:** This matrix connects the $3$ hidden layer 2 neurons to the $1$ output neuron. Each element $w^{(3)}_{j}$ represents the weight from layer2 neuron $j$ 1～3 to the single output neuron.\n",
    "*   **Output Activation (Sigmoid):**\n",
    "    \\[\n",
    "    $\\mathbf{A}^{[Last]} = \\mathbf{\\hat{Y}} = \\sigma(\\mathbf{Z}^{[3]}) = \\frac{1}{1 + e^{-\\mathbf{Z}^{[3]}}}$\n",
    "    \\]\n",
    "    Applies the sigmoid function to $\\mathbf{Z}^{[3]}$ to produce the final predictions $\\mathbf{\\hat{Y}}$. Each element $\\hat{y}_i$ represents a probability between 0 and 1. $\\mathbf{\\hat{Y}}$ has shape $(n, 1)$, $\\sum(\\hat{y}_i) = \\sum(Ypred) = 1$.\n",
    "\n",
    "*Where:*\n",
    "- $m$ is the sample size (Instance number)\n",
    "- $\\sigma$ = activation function for hidden layers (e.g., ReLU, sigmoid, tanh)\n",
    "- $\\sigma_{\\text{out}}$ = output layer activation function (e.g., linear for regression, sigmoid for binary classification)\n",
    "- $^\\top$ = transpose operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbaaf52",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\" valign=\"top\">\n",
    "\n",
    "# Gradient Decent Process \n",
    "\n",
    "In Neural Networks Context we have a **loss function** $ L $, which is a function of all weights $W_1, W_2$.\n",
    "\n",
    "The gradient of loss function is the derivative of $L$ with respect to model parameters $w$ $\\frac{\\partial L}{\\partial W}$.\n",
    "\n",
    "What the Gradient Tells Us:\n",
    "- **Rate of change**: How fast the loss function changes in each weight direction\n",
    "- **Magnitude of adjustment**: How much each weight should be adjusted\n",
    "- **Direction of adjustment**: Whether each weight should be increased or decreased\n",
    "\n",
    "> **Key Point**:    \n",
    "> 1.The gradient points in the direction of the **steepest increase** of the loss function.  \n",
    "> 2.Since our objective is to **minimize** the loss, we adjust the weights in the **opposite direction** of the gradient.  \n",
    "> 3.Gradient = 0 at min $l$ (a flat slope): $L$ won’t change for smalls changes in $w$ .\n",
    "\n",
    "\n",
    "\n",
    "</td>\n",
    "<td width=\"20%\" valign=\"top\">\n",
    "\n",
    "![Network Architecture](gradient.png)\n",
    "It measures how fast $L$ changes for a change in parameter $w$.  (i.e., slope of the surface) \n",
    "\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce3ae2",
   "metadata": {},
   "source": [
    "## How is the Gradient Calculated? \n",
    "\n",
    "### Partial Derivatives\n",
    "\n",
    "A **partial derivative** measures the rate of change of a multivariable function with respect to one variable, while holding others constant.\n",
    "\n",
    "**Example**: Room temperature $T$ depends on:\n",
    "- AC setting $x1$\n",
    "- Outdoor temperature $x2$\n",
    "\n",
    "Then:\n",
    "- $\\frac{\\partial T}{\\partial x1}$ represents: When outdoor temperature $x2$ is fixed, how much does indoor temperature change per unit increase in AC setting?\n",
    "- $\\frac{\\partial T}{\\partial x2}$ represents: When AC setting $x1$ is fixed, how much does indoor temperature change per unit increase in outdoor temperature?\n",
    "\n",
    "### Chain Rule\n",
    "Neural network computations are nested layer by layer, so we need the **chain rule**.\n",
    "\n",
    "**Example**: $\\hat{y} = f(g(x))$\n",
    "\n",
    "Then: $\\frac{dy}{dx} = \\frac{dy}{dg} \\times \\frac{dg}{dx}$\n",
    "\n",
    "In neural networks, we use the chain rule to propagate gradients backward through the network—this is called **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78d1ec",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td width=\"60%\" valign=\"top\">\n",
    "\n",
    "## Scenario: House Price Prediction\n",
    "\n",
    "We aim to predict house prices based on two features:\n",
    "\n",
    "1. **House Area** (square meters)\n",
    "2. **House Age** (years)\n",
    "\n",
    "We create a **2-layer neural network** to process the data and make predictions:\n",
    "- **Hidden layer**: 3 neurons\n",
    "- **Output layer**: 1 neuron\n",
    "\n",
    "### Weight Vector\n",
    "$\\mathbf{W}^{[1]} = \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\\\ 0.5 & 0.6 \\end{bmatrix}$  \n",
    "$\\mathbf{W}^{[2]} = \\begin{bmatrix} 0.7 & 0.8 & 0.9 \\end{bmatrix}$\n",
    "\n",
    "### Bias Vector\n",
    "$\\mathbf{b}^{[1]} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 \\end{bmatrix}$  \n",
    "$\\mathbf{b}^{[2]} = \\begin{bmatrix} 0.5 \\end{bmatrix}$\n",
    "\n",
    "</td>\n",
    "<td width=\"40%\" valign=\"top\">\n",
    "\n",
    "![Network Architecture](gradient2.png)\n",
    "\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5df2d",
   "metadata": {},
   "source": [
    "step 1 数据（scaled）输入 [2.0,1.0]\n",
    "\n",
    "step 2 隐藏层神经元计算\n",
    "1.hidden layer 线形转换 $\\mathbf{Z}^{[1]}$ = [ 2.0, 1.0 ] x $\\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\\\ 0.5 & 0.6 \\end{bmatrix} ^\\top $ + $\\begin{bmatrix} 0.1 & 0.2 & 0.3 \\end{bmatrix}$ = [ 0.5, 1.2, 1.9 ]  \n",
    "2.hidden layer activation $\\mathbf{A}^{[1]} = \\sigma(\\mathbf{Z}^{[1]})$ = [ 0.62, 0.77, 0.87]\n",
    "\n",
    "step 3 输出层神经元计算\n",
    "1.output layer linear transfer $\\mathbf{Z}^{[2]}$ = [ 0.62, 0.77, 0.87] x $\\begin{bmatrix} 0.7 & 0.8 & 0.9  \\end{bmatrix} ^\\top $ + $\\begin{bmatrix} 0.5 \\end{bmatrix}$ = [ 1.833 ]  \n",
    "2.output layer activation $\\mathbf{A}^{[last]} = \\sigma(\\mathbf{Z}^{[2]})$ = [ 0.91 ]\n",
    "\n",
    "step 4 损失函数计算  \n",
    "1.选择MSE作为损失函数 $ L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2$  \n",
    "2.定义学习率 $n$ = 0.1\n",
    "\n",
    "step 5 梯度推导流程\n",
    "  1. 前向传播（计算预测值）\n",
    "     输入 → 隐藏层计算 → 输出层计算 → 预测值\n",
    "  \n",
    "  2. 计算损失\n",
    "     损失$L$ = $\\frac{1}{2}$(预测值 - 真实值)²\n",
    "  \n",
    "  3. 反向传播\n",
    "     计算每层各个权重的梯度（责任）\n",
    "  \n",
    "  4. 更新权重\n",
    "     新权重 = 旧权重 - 学习率 × 梯度\n",
    "  \n",
    "  5. 重复直至最优\n",
    "结束训练\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7736f35",
   "metadata": {},
   "source": [
    "## Neural Network Forward Propagation Example\n",
    "\n",
    "### Step 1: Input Data (Scaled)\n",
    "**Input**: $\\mathbf{X} = \\begin{bmatrix} 2.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "### Step 2: Hidden Layer Computation\n",
    "\n",
    "#### 1. Linear Transformation in Hidden Layer\n",
    "$$\n",
    "\\mathbf{Z}^{[1]} = \\mathbf{X} \\cdot (\\mathbf{W}^{[1]})^\\top + \\mathbf{b}^{[1]}\n",
    "$$\n",
    "\n",
    "Calculation:\n",
    "$$\n",
    "\\mathbf{Z}^{[1]} = \\begin{bmatrix} 2.0 & 1.0 \\end{bmatrix} \\times \n",
    "\\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\\\ 0.5 & 0.6 \\end{bmatrix}^\\top + \n",
    "\\begin{bmatrix} 0.1 & 0.2 & 0.3 \\end{bmatrix} \n",
    "= \\begin{bmatrix} 0.5 & 1.2 & 1.9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 2. Activation in Hidden Layer\n",
    "Using sigmoid activation $\\sigma$:\n",
    "$$\n",
    "\\mathbf{A}^{[1]} = \\sigma(\\mathbf{Z}^{[1]}) = \\begin{bmatrix} 0.62 & 0.77 & 0.87 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Step 3: Output Layer Computation\n",
    "\n",
    "#### 1. Linear Transformation in Output Layer\n",
    "$$\n",
    "\\mathbf{Z}^{[2]} = \\mathbf{A}^{[1]} \\cdot (\\mathbf{W}^{[2]})^\\top + \\mathbf{b}^{[2]}\n",
    "$$\n",
    "\n",
    "Calculation:\n",
    "$$\n",
    "\\mathbf{Z}^{[2]} = \\begin{bmatrix} 0.62 & 0.77 & 0.87 \\end{bmatrix} \\times \n",
    "\\begin{bmatrix} 0.7 & 0.8 & 0.9 \\end{bmatrix}^\\top + \n",
    "\\begin{bmatrix} 0.5 \\end{bmatrix} \n",
    "= \\begin{bmatrix} 1.833 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 2. Activation in Output Layer\n",
    "$$\n",
    "\\mathbf{A}^{[2]} = \\sigma(\\mathbf{Z}^{[2]}) = \\begin{bmatrix} 0.91 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Final Prediction**: $\\hat{y} = 0.91$\n",
    "\n",
    "### Step 4: Loss Function Computation\n",
    "\n",
    "#### 1. Mean Squared Error (MSE) Loss\n",
    "$$\n",
    "L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
    "$$\n",
    "\n",
    "#### 2. Learning Rate\n",
    "Learning rate: $\\eta = 0.1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeede41",
   "metadata": {},
   "source": [
    "### Step 5: Gradient Descent Training Pipeline\n",
    "\n",
    "#### 1. Forward Propagation\n",
    "Input → Hidden Layer Computation → Output Layer Computation → Prediction\n",
    "\n",
    "#### 2. Loss Calculation\n",
    "Compute loss: $L = \\frac{1}{2}(\\hat{y} - y)^2$\n",
    "\n",
    "#### 3. Backward Propagation\n",
    "Calculate gradients for each weight in every layer (responsibility assignment)\n",
    "z1 → a1 → z2 → y_pred(a2) → L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6fe7f",
   "metadata": {},
   "source": [
    "### 3.1 Compute Output Layer Gradients\n",
    "\n",
    "$$\n",
    "y_{\\text{pred}} = \\sigma(z^{[2]})\n",
    "$$\n",
    "\n",
    "#### Chain Rule Application\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z^{[2]}} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial z^{[2]}}\n",
    "$$\n",
    "\n",
    "#### Step 1: Derivative of Prediction\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_{\\text{pred}}} = -(y_{\\text{true}} - y_{\\text{pred}}) = -(1.0 - 0.91) = -0.09\n",
    "$$\n",
    "\n",
    "#### Step 2: Derivative of Sigmoid\n",
    "$$\n",
    "\\frac{\\partial y_{\\text{pred}}}{\\partial z^{[2]}} = y_{\\text{pred}} \\times (1 - y_{\\text{pred}})\n",
    "$$\n",
    "$$\n",
    "= 0.91 \\times (1 - 0.91) = 0.91 \\times 0.09 = 0.0819\n",
    "$$\n",
    "\n",
    "#### Step 3: Output Layer Error Signal\n",
    "**$\\frac{\\partial L}{\\partial z^{[2]}}$ (Output layer error signal)**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z^{[2]}} = (-0.09) \\times 0.0819 = -0.007371\n",
    "$$\n",
    "\n",
    "#### Step 4: Gradient of Output Layer Weights\n",
    "$$\n",
    "z^{[2]} = \\mathbf{W}^{[2]} \\cdot \\mathbf{a}^{[1]} + b^{[2]}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{[2]}} = \\frac{\\partial L}{\\partial z^{[2]}} \\times \\frac{\\partial z^{[2]}}{\\partial \\mathbf{W}^{[2]}} = -0.007371 \\times \\mathbf{a}^{[1]}\n",
    "$$\n",
    "$$\n",
    "= -0.007371 \\times [0.62, 0.77, 0.87] = [-0.004570, -0.005676, -0.006413]\n",
    "$$\n",
    "\n",
    "#### Summary:\n",
    "- **First weight $w_{21}$**: For every unit increase, loss decreases by 0.00457\n",
    "- **Second weight $w_{22}$**: For every unit increase, loss decreases by 0.00568\n",
    "- **Third weight $w_{23}$**: For every unit increase, loss decreases by 0.00641"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd4334",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 3.2 Compute Hidden Layer Gradients\n",
    "\n",
    "#### Network Equations\n",
    "$$\n",
    "\\mathbf{z}^{[1]} = \\mathbf{W}^{[1]} \\cdot \\mathbf{X} + \\mathbf{b}^{[1]}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{a}^{[1]} = \\sigma(\\mathbf{z}^{[1]})\n",
    "$$\n",
    "\n",
    "#### Chain Rule Application\n",
    "**$\\frac{\\partial L}{\\partial \\mathbf{z}^{[1]}}$ (Hidden layer error signal)**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{[1]}} = \\frac{\\partial L}{\\partial z^{[2]}} \\times \\frac{\\partial z^{[2]}}{\\partial \\mathbf{a}^{[1]}} \\times \\frac{\\partial \\mathbf{a}^{[1]}}{\\partial \\mathbf{z}^{[1]}} \\times \\frac{\\partial \\mathbf{z}^{[1]}}{\\partial \\mathbf{W}^{[1]}}\n",
    "$$\n",
    "\n",
    "#### Step-by-Step Computation\n",
    "1. **Output layer error**: $\\frac{\\partial L}{\\partial z^{[2]}} = -0.007371$\n",
    "2. **Gradient through weights**: $\\frac{\\partial z^{[2]}}{\\partial \\mathbf{a}^{[1]}} = \\mathbf{W}^{[2]}$\n",
    "3. **Sigmoid derivative**: $\\frac{\\partial \\mathbf{a}^{[1]}}{\\partial \\mathbf{z}^{[1]}} = \\sigma'(\\mathbf{z}^{[1]}) = \\mathbf{z}^{[1]} \\odot (1 - \\mathbf{z}^{[1]})$\n",
    "4. **Input contribution**: $\\frac{\\partial \\mathbf{z}^{[1]}}{\\partial \\mathbf{W}^{[1]}} = \\mathbf{X}$\n",
    "\n",
    "#### Calculation\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{[1]}} = -0.007371 \\times \\mathbf{W}^{[2]^\\top} \\odot \\sigma'(\\mathbf{z}^{[1]}) \\times \\mathbf{X}\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbf{W}^{[2]^\\top} = [0.7, 0.8, 0.9]^\\top$\n",
    "- $\\sigma'(\\mathbf{z}^{[1]}) = [0.2356, 0.1771, 0.1131]^\\top$\n",
    "- $\\mathbf{X} = [2.0, 1.0]$\n",
    "\n",
    "#### Result\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{[1]}} = \n",
    "\\begin{bmatrix}\n",
    "-0.002430 & -0.002088 & -0.001500 \\\\\n",
    "-0.001215 & -0.001044 & -0.000750\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Interpretation:\n",
    "Each element represents how much the loss changes with respect to a small change in the corresponding hidden layer weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19906e07",
   "metadata": {},
   "source": [
    "### 4. Weight Update\n",
    "New weight = Old weight − Learning rate × Gradient\n",
    "\n",
    "### 5. Iterate Until Optimal\n",
    "Repeat steps 1-4 until convergence or reaching optimal solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
